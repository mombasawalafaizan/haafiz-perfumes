#!/usr/bin/env node

/**
 * Supabase Database Backup Script (No Docker Required)
 * Uses Supabase REST API to export data
 */

import fs from "fs";
import path from "path";
import https from "https";
import { gzip } from "zlib";
import { promisify } from "util";

// Load environment variables from .env.local
function loadEnvFile() {
  const envPath = path.join(process.cwd(), ".env.local");
  if (fs.existsSync(envPath)) {
    const envContent = fs.readFileSync(envPath, "utf8");
    const envVars = {};
    envContent.split("\n").forEach((line) => {
      const [key, ...valueParts] = line.split("=");
      if (key && valueParts.length > 0) {
        envVars[key.trim()] = valueParts.join("=").trim();
      }
    });
    return envVars;
  }
  return {};
}

const env = loadEnvFile();

// Configuration
const config = {
  supabaseUrl: env.SUPABASE_URL,
  supabaseAnonKey: env.SUPABASE_ANON_KEY,
  backupDir: "./backups",
  maxBackups: 7,
};

// Colors for console output
const colors = {
  reset: "\x1b[0m",
  red: "\x1b[31m",
  green: "\x1b[32m",
  yellow: "\x1b[33m",
  blue: "\x1b[34m",
  cyan: "\x1b[36m",
};

function log(message, color = "reset") {
  console.log(`${colors[color]}${message}${colors.reset}`);
}

function makeRequest(url, options = {}) {
  return new Promise((resolve, reject) => {
    const req = https.request(url, options, (res) => {
      let data = "";
      res.on("data", (chunk) => (data += chunk));
      res.on("end", () => {
        try {
          const jsonData = JSON.parse(data);
          resolve({ status: res.statusCode, data: jsonData });
        } catch (error) {
          log(`❌ Error parsing JSON: ${error.message}`, "red");
          resolve({ status: res.statusCode, data: data });
        }
      });
    });

    req.on("error", reject);
    req.end();
  });
}

async function getTableData(tableName) {
  log(`📦 Fetching data from ${tableName}...`, "blue");

  const url = `${config.supabaseUrl}/rest/v1/${tableName}?select=*`;
  const options = {
    headers: {
      apikey: config.supabaseAnonKey,
      Authorization: `Bearer ${config.supabaseAnonKey}`,
      "Content-Type": "application/json",
    },
  };

  try {
    const response = await makeRequest(url, options);

    if (response.status !== 200) {
      throw new Error(`Failed to fetch ${tableName}: ${response.status}`);
    }

    return response.data;
  } catch (error) {
    log(`❌ Error fetching ${tableName}: ${error.message}`, "red");
    return [];
  }
}

async function getTableSchema(tableName) {
  log(`🔍 Getting schema for ${tableName}...`, "cyan");

  // This is a simplified schema export
  // In a real implementation, you'd query the information_schema
  return `-- Table: ${tableName}
-- Schema export for ${tableName}
-- Generated on ${new Date().toISOString()}

`;
}

async function exportTable(tableName) {
  const schema = await getTableSchema(tableName);
  const data = await getTableData(tableName);

  let sql = schema;

  if (data.length > 0) {
    // Get column names from first row
    const columns = Object.keys(data[0]);
    const columnNames = columns.join(", ");

    sql += `-- Data for ${tableName}\n`;
    sql += `-- ${data.length} rows\n\n`;

    // Generate INSERT statements
    for (const row of data) {
      const values = columns
        .map((col) => {
          const value = row[col];
          if (value === null) return "NULL";
          if (typeof value === "string")
            return `'${value.replace(/'/g, "''")}'`;
          if (typeof value === "boolean") return value ? "TRUE" : "FALSE";
          if (typeof value === "object")
            return `'${JSON.stringify(value).replace(/'/g, "''")}'`;
          return value;
        })
        .join(", ");

      sql += `INSERT INTO ${tableName} (${columnNames}) VALUES (${values});\n`;
    }
  } else {
    sql += `-- No data found in ${tableName}\n`;
  }

  sql += "\n";
  return sql;
}

async function createBackup() {
  const timestamp = new Date()
    .toISOString()
    .replace(/[:.]/g, "-")
    .split("T")[0];
  const backupFile = `haafiz_perfumes_backup_${timestamp}.sql`;
  const backupPath = path.join(config.backupDir, backupFile);

  // Create backup directory
  if (!fs.existsSync(config.backupDir)) {
    fs.mkdirSync(config.backupDir, { recursive: true });
  }

  log("🔄 Starting database backup...", "yellow");

  // Check environment variables
  if (!config.supabaseUrl || !config.supabaseAnonKey) {
    log(
      "❌ Missing Supabase credentials. Please set SUPABASE_URL and SUPABASE_ANON_KEY",
      "red"
    );
    process.exit(1);
  }

  let sql = `-- Haafiz Perfumes Database Backup
-- Generated on: ${new Date().toISOString()}
-- Generated by: Node.js Backup Script (No Docker Required)

`;

  // List of tables to backup (all tables from your Supabase database)
  const tables = [
    "products",
    "orders",
    "order_items",
    "hero_slides",
    "images",
    "product_images",
    "product_variants",
    "variant_images",
  ];

  for (const table of tables) {
    try {
      const tableSQL = await exportTable(table);
      sql += tableSQL;
      log(`✅ Exported ${table}`, "green");
    } catch (error) {
      log(`❌ Failed to export ${table}: ${error.message}`, "red");
    }
  }

  // Write backup file
  fs.writeFileSync(backupPath, sql);
  log(`✅ Backup created: ${backupPath}`, "green");

  // Compress backup
  const gzipAsync = promisify(gzip);

  try {
    const compressed = await gzipAsync(sql);
    const compressedPath = `${backupPath}.gz`;
    fs.writeFileSync(compressedPath, compressed);
    log(`🗜️ Compressed backup: ${compressedPath}`, "green");

    // Remove uncompressed file
    fs.unlinkSync(backupPath);
  } catch (error) {
    log(`⚠️ Could not compress backup: ${error.message}`, "yellow");
  }

  // Clean old backups
  try {
    const files = fs
      .readdirSync(config.backupDir)
      .filter(
        (file) =>
          file.startsWith("haafiz_perfumes_backup_") && file.endsWith(".sql.gz")
      )
      .sort()
      .reverse();

    if (files.length > config.maxBackups) {
      const filesToDelete = files.slice(config.maxBackups);
      for (const file of filesToDelete) {
        fs.unlinkSync(path.join(config.backupDir, file));
        log(`🗑️ Deleted old backup: ${file}`, "yellow");
      }
    }
  } catch (error) {
    log(`⚠️ Could not clean old backups: ${error.message}`, "yellow");
  }

  log("🎉 Backup completed successfully!", "green");
}

// Run backup
if (import.meta.url === `file://${process.argv[1]}`) {
  createBackup().catch((error) => {
    log(`❌ Backup failed: ${error.message}`, "red");
    process.exit(1);
  });
}

export { createBackup };
